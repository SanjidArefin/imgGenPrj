{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 8196,
     "status": "ok",
     "timestamp": 1762930317186,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "p_o6N5JxoIoJ",
    "outputId": "26f9935c-2f58-45ee-c18e-95f27052a959"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install scikit-image opencv-python torch torchvision ffmpeg-python scikit-video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 16108,
     "status": "ok",
     "timestamp": 1762930333298,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "KxSrkdJzotF3",
    "outputId": "866ca2e7-bb9a-4ff6-ccf9-8c8c076000c0"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!apt-get install p7zip-full -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 6827,
     "status": "ok",
     "timestamp": 1762930340128,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "jMxuw6lWou7t"
   },
   "outputs": [],
   "source": [
    "import os, sys, cv2, glob, torch, numpy as np\n",
    "from google.colab import drive\n",
    "from skimage.metrics import structural_similarity as ssim_metric\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1211,
     "status": "ok",
     "timestamp": 1762930341341,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "zc7Ynoglo3nA",
    "outputId": "895c1186-6f42-46aa-acf9-3fa661f7e565"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/hzwer/arXiv2020-RIFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 4840,
     "status": "ok",
     "timestamp": 1762930346182,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "pt_UtUVlo4tE",
    "outputId": "d6f11982-dfbf-48e1-feb3-7052ed806cab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1APIzVeI-4ZZCEuIRE1m6WYfSCaOsi_7_\n",
      "To: /content/RIFE_trained_model_v3.6.zip\n",
      "100% 11.3M/11.3M [00:00<00:00, 27.3MB/s]\n",
      "\n",
      "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
      "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.00GHz (50653),ASM,AES-NI)\n",
      "\n",
      "Scanning the drive for archives:\n",
      "  0M Scan\b\b\b\b\b\b\b\b\b         \b\b\b\b\b\b\b\b\b1 file, 11332064 bytes (11 MiB)\n",
      "\n",
      "Extracting archive: RIFE_trained_model_v3.6.zip\n",
      "--\n",
      "Path = RIFE_trained_model_v3.6.zip\n",
      "Type = zip\n",
      "Physical Size = 11332064\n",
      "\n",
      "  0%\b\b\b\b    \b\b\b\b 40% 5 - train_log/flownet.pkl\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok\n",
      "\n",
      "Folders: 4\n",
      "Files: 10\n",
      "Size:       12208819\n",
      "Compressed: 11332064\n"
     ]
    }
   ],
   "source": [
    "!gdown --id 1APIzVeI-4ZZCEuIRE1m6WYfSCaOsi_7_\n",
    "!7z e RIFE_trained_model_v3.6.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 13668,
     "status": "ok",
     "timestamp": 1762930359869,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "BJO9WmgvpDbs",
    "outputId": "97464941-a9ba-462c-a092-a458884fbe4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/arXiv2020-RIFE/train_log\n",
      "/content/arXiv2020-RIFE\n",
      "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1i3xlKb7ax7Y70khcTcuePi6E7crO_dFc\n",
      "To: /content/arXiv2020-RIFE/demo.mp4\n",
      "100% 54.6M/54.6M [00:01<00:00, 51.0MB/s]\n",
      "Collecting git+https://github.com/rk-exxec/scikit-video.git@numpy_deprecation\n",
      "  Cloning https://github.com/rk-exxec/scikit-video.git (to revision numpy_deprecation) to /tmp/pip-req-build-4ir0_f1q\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/rk-exxec/scikit-video.git /tmp/pip-req-build-4ir0_f1q\n",
      "  Running command git checkout -b numpy_deprecation --track origin/numpy_deprecation\n",
      "  Switched to a new branch 'numpy_deprecation'\n",
      "  Branch 'numpy_deprecation' set up to track remote branch 'numpy_deprecation' from 'origin'.\n",
      "  Resolved https://github.com/rk-exxec/scikit-video.git to commit 74cbbb2e19599304bf069529537b23a518fdc3c9\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from scikit-video==1.1.11) (2.0.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from scikit-video==1.1.11) (1.16.3)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from scikit-video==1.1.11) (11.3.0)\n"
     ]
    }
   ],
   "source": [
    "!mkdir /content/arXiv2020-RIFE/train_log\n",
    "%cd /content/arXiv2020-RIFE/train_log\n",
    "%cd /content/arXiv2020-RIFE/\n",
    "!gdown --id 1i3xlKb7ax7Y70khcTcuePi6E7crO_dFc\n",
    "!pip install git+https://github.com/rk-exxec/scikit-video.git@numpy_deprecation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 45934,
     "status": "ok",
     "timestamp": 1762930405806,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "1eYMGNdQpFgP",
    "outputId": "628dedc9-0dc0-49a8-8756-ffd78c76db94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "all_generated_frames\t   man_drinking_short.mp4\n",
      "all_generated_frames_pure  ssim_iterations\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')\n",
    "!ls /content/drive/MyDrive/imgGenPrj_project/resource\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1762930405811,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "uhuWJluOrJUS"
   },
   "outputs": [],
   "source": [
    "video = '/content/drive/MyDrive/imgGenPrj_project/resource/man_drinking_short.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1762930405828,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "8ULHTM65qHuq"
   },
   "outputs": [],
   "source": [
    "# Make output folder\n",
    "\n",
    "output_folder = '/content/drive/MyDrive/imgGenPrj_project/resource'\n",
    "os.makedirs(output_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1133,
     "status": "ok",
     "timestamp": 1762930406960,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "WBZy9siQqhUs",
    "outputId": "63a70562-e378-4cee-9413-a0516b7fd4fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS         : 25 \n",
      "Total Frames: 125\n"
     ]
    }
   ],
   "source": [
    "# Frame calculation\n",
    "\n",
    "cap = cv2.VideoCapture(video)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "total_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(f\"FPS         : {fps} \\nTotal Frames: {total_frame}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1762930407243,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "wqqIAC4Nrvp1",
    "outputId": "690af09a-8df7-4661-9c74-2df0c4335820"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ff.jpg saved: /content/drive/MyDrive/imgGenPrj_project/resource/ff.jpg\n",
      "fm.jpg saved: /content/drive/MyDrive/imgGenPrj_project/resource/fm.jpg\n",
      "fl.jpg saved: /content/drive/MyDrive/imgGenPrj_project/resource/fl.jpg\n"
     ]
    }
   ],
   "source": [
    "# Extract ff,fm,fl\n",
    "import cv2, os\n",
    "\n",
    "def save_frame(position, name):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, position)\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        path = os.path.join(output_folder, name)\n",
    "        cv2.imwrite(path, frame)\n",
    "        print(f\"{name} saved: {path}\")\n",
    "        return path\n",
    "    else:\n",
    "        print(f\" Failed to read frame at position {position}\")\n",
    "    return None\n",
    "\n",
    "# Use min(total_frame-2, total_frame-1) to stay within valid range\n",
    "first_frame_path = save_frame(0, \"ff.jpg\")\n",
    "middle_frame_path = save_frame(total_frame // 2, \"fm.jpg\")\n",
    "last_frame_path = save_frame(-1, \"fl.jpg\")  # <- safer for end of video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1762930407280,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "eskbkPBKt7tz"
   },
   "outputs": [],
   "source": [
    "# load frame\n",
    "ff = cv2.imread(first_frame_path)\n",
    "fl = cv2.imread(last_frame_path)\n",
    "fm = cv2.imread(middle_frame_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 32793,
     "status": "ok",
     "timestamp": 1762930563793,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "aS8hv8cAtXBO",
    "outputId": "2dcc2af5-7496-4a96-d576-c899a7d11c31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded v3.x HD model.\n",
      "/content/arXiv2020-RIFE/man_drinking_short.mp4, 125.0 frames in total, 25.0FPS to 50.0FPS\n",
      "The audio will be merged after interpolation process\n",
      " 99% 124/125.0 [00:17<00:00,  7.19it/s]\n",
      "error: XDG_RUNTIME_DIR not set in the environment.\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/content/arXiv2020-RIFE/man_drinking_short.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    creation_time   : 2025-11-12T06:50:52.000000Z\n",
      "    encoder         : Blackmagic Design DaVinci Resolve Studio\n",
      "  Duration: 00:00:05.00, start: 0.000000, bitrate: 4682 kb/s\n",
      "  Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709), 1280x720 [SAR 1:1 DAR 16:9], 4559 kb/s, 25 fps, 25 tbr, 12800 tbn, 50 tbc (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2025-11-12T06:50:52.000000Z\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : H.264 AMD\n",
      "      timecode        : 01:00:00:00\n",
      "  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 117 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2025-11-12T06:50:52.000000Z\n",
      "      handler_name    : SoundHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:2(eng): Data: none (tmcd / 0x64636D74), 0 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2025-11-12T06:50:52.000000Z\n",
      "      handler_name    : TimeCodeHandler\n",
      "      timecode        : 01:00:00:00\n",
      "Output #0, matroska, to './temp/audio.mkv':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf58.76.100\n",
      "  Stream #0:0(und): Audio: aac (LC) ([255][0][0][0] / 0x00FF), 48000 Hz, stereo, fltp, 117 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2025-11-12T06:50:52.000000Z\n",
      "      handler_name    : SoundHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (copy)\n",
      "Press [q] to stop, [?] for help\n",
      "size=      74kB time=00:00:04.99 bitrate= 121.6kbits/s speed=4.7e+03x    \n",
      "video:0kB audio:72kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 3.226202%\n",
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/content/arXiv2020-RIFE/man_drinking_short_2X_50fps_noaudio.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf59.27.100\n",
      "  Duration: 00:00:04.98, start: 0.000000, bitrate: 5099 kb/s\n",
      "  Stream #0:0(und): Video: mpeg4 (Simple Profile) (mp4v / 0x7634706D), yuv420p, 1280x720 [SAR 1:1 DAR 16:9], 5096 kb/s, 50 fps, 50 tbr, 12800 tbn, 50 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Input #1, matroska,webm, from './temp/audio.mkv':\n",
      "  Metadata:\n",
      "    COMPATIBLE_BRANDS: isomiso2avc1mp41\n",
      "    MAJOR_BRAND     : isom\n",
      "    MINOR_VERSION   : 512\n",
      "    ENCODER         : Lavf58.76.100\n",
      "  Duration: 00:00:05.01, start: 0.000000, bitrate: 121 kb/s\n",
      "  Stream #1:0: Audio: aac (LC), 48000 Hz, stereo, fltp (default)\n",
      "    Metadata:\n",
      "      HANDLER_NAME    : SoundHandler\n",
      "      VENDOR_ID       : [0][0][0][0]\n",
      "      DURATION        : 00:00:05.013000000\n",
      "Output #0, mp4, to '/content/arXiv2020-RIFE/man_drinking_short_2X_50fps.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf58.76.100\n",
      "  Stream #0:0(und): Video: mpeg4 (Simple Profile) (mp4v / 0x7634706D), yuv420p, 1280x720 [SAR 1:1 DAR 16:9], q=2-31, 5096 kb/s, 50 fps, 50 tbr, 12800 tbn, 12800 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1: Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp (default)\n",
      "    Metadata:\n",
      "      HANDLER_NAME    : SoundHandler\n",
      "      VENDOR_ID       : [0][0][0][0]\n",
      "      DURATION        : 00:00:05.013000000\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (copy)\n",
      "  Stream #1:0 -> #0:1 (copy)\n",
      "Press [q] to stop, [?] for help\n",
      "frame=  249 fps=0.0 q=-1.0 Lsize=    3176kB time=00:00:04.99 bitrate=5211.4kbits/s speed= 610x    \n",
      "video:3098kB audio:72kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.176052%\n"
     ]
    }
   ],
   "source": [
    "# load RIFE model\n",
    "!python3 inference_video.py --exp=1 --video=/content/arXiv2020-RIFE/man_drinking_short.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1762930568999,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "ITjoho7lDTp3",
    "outputId": "4ee0ed4b-1c94-47e0-8ace-56cb9109c6f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1762930570901,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "WzzBEyR5ECUb",
    "outputId": "dce44ed0-b7ae-4902-b463-541138bf916e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/imgGenPrj_project/resource/ff.jpg True\n",
      "/content/drive/MyDrive/imgGenPrj_project/resource/fl.jpg True\n",
      "/content/drive/MyDrive/imgGenPrj_project/resource/fm.jpg True\n"
     ]
    }
   ],
   "source": [
    "print(first_frame_path, os.path.exists(first_frame_path))\n",
    "print(last_frame_path, os.path.exists(last_frame_path))\n",
    "print(middle_frame_path, os.path.exists(middle_frame_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1762930572488,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "0ZGbx3qpEEjV",
    "outputId": "0bf282d1-db06-4cee-ea31-3fba2525f3fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 1280, 3) (720, 1280, 3) (720, 1280, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Read images as arrays (not paths)\n",
    "first_frame = cv2.imread(first_frame_path)   # shape (H,W,3)\n",
    "last_frame  = cv2.imread(last_frame_path)\n",
    "middle_frame = cv2.imread(middle_frame_path)\n",
    "\n",
    "# Check shapes\n",
    "print(first_frame.shape, last_frame.shape, middle_frame.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1762930574550,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "Y3yEhOROEIjv",
    "outputId": "0bdcab81-c81c-4837-85f6-458fef1cd54d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Convert to tensors\n",
    "first_tensor = torch.from_numpy(first_frame.transpose(2,0,1)).unsqueeze(0).float() / 255.0\n",
    "last_tensor  = torch.from_numpy(last_frame.transpose(2,0,1)).unsqueeze(0).float() / 255.0\n",
    "\n",
    "# Move to device\n",
    "first_tensor = first_tensor.to(device)\n",
    "last_tensor  = last_tensor.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1762930577204,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "6uoLFQX2ENoL"
   },
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim_metric\n",
    "\n",
    "def compute_ssim(gen, ref):\n",
    "    gen = cv2.resize(gen, (ref.shape[1], ref.shape[0]))\n",
    "    return ssim_metric(cv2.cvtColor(gen, cv2.COLOR_BGR2GRAY),\n",
    "                       cv2.cvtColor(ref, cv2.COLOR_BGR2GRAY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1320,
     "status": "ok",
     "timestamp": 1762930580528,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "HlKAl8MkEdh0",
    "outputId": "94ec0067-9541-475f-b8d3-7ae559d8d6f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "RIFE model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add RIFE code folder to path\n",
    "sys.path.append('/content/arXiv2020-RIFE')\n",
    "\n",
    "from train_log.RIFE_HDv3 import Model  # or use RIFE_HDv3 if you have it\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize and load model\n",
    "model = Model()\n",
    "model.load_model('./train_log', -1)  # replace './train_log' with your model weights path\n",
    "model.eval()\n",
    "model.device()  # set model to device\n",
    "print(\"RIFE model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 5677,
     "status": "ok",
     "timestamp": 1762930588365,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "2BMU09JKQQqu",
    "outputId": "f8275981-83f8-419b-dd97-bf80a5ba7197"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SSIM optimization with hybrid refinement...\n",
      "Fixing tensor dimensions...\n",
      "Corrected tensor dimensions: 1280x704\n",
      "first_tensor shape: torch.Size([1, 3, 704, 1280])\n",
      "last_tensor shape: torch.Size([1, 3, 704, 1280])\n",
      "\n",
      "[Phase 1] Finding best frame pair...\n",
      "  Try 1: offset= +0, SSIM=0.6704\n",
      "    ✓ New best base!\n",
      "  Try 2: offset= -5, SSIM=0.6704\n",
      "  Try 3: offset= +5, SSIM=0.6657\n",
      "  Try 4: offset=-10, SSIM=0.6704\n",
      "    ✓ New best base!\n",
      "  Try 5: offset=+10, SSIM=0.6517\n",
      "  Try 6: offset=-15, SSIM=0.6704\n",
      "  Try 7: offset=+15, SSIM=0.6397\n",
      "  Try 8: offset=-20, SSIM=0.6704\n",
      "    ✓ New best base!\n",
      "  Try 9: offset=+20, SSIM=0.6167\n",
      "\n",
      "[Phase 1 Complete] Best base SSIM: 0.6704\n",
      "  Best offset: -20\n",
      "\n",
      "[Phase 2] Progressive refinement towards target...\n",
      "  Iteration 2: alpha=0.02, SSIM=0.6770 (Δ=+0.0067)\n",
      "  Iteration 3: alpha=0.04, SSIM=0.6933 (Δ=+0.0163)\n",
      "  Iteration 4: alpha=0.06, SSIM=0.7182 (Δ=+0.0249)\n",
      "  Iteration 5: alpha=0.08, SSIM=0.7506 (Δ=+0.0324)\n",
      "  Iteration 6: alpha=0.10, SSIM=0.7881 (Δ=+0.0375)\n",
      "  Iteration 7: alpha=0.12, SSIM=0.8269 (Δ=+0.0387)\n",
      "  Iteration 8: alpha=0.15, SSIM=0.8682 (Δ=+0.0413)\n",
      "  Iteration 9: alpha=0.18, SSIM=0.9075 (Δ=+0.0393)\n",
      "  ✓ Threshold reached!\n",
      "\n",
      "[Phase 2 Complete] Final SSIM: 0.9075\n",
      "\n",
      "============================================================\n",
      "SSIM OPTIMIZATION COMPLETE\n",
      "============================================================\n",
      "Final SSIM: 0.9075\n",
      "Improvement from base: +0.2372\n",
      "✓ SSIM threshold reached! Frame saved at: /content/drive/MyDrive/imgGenPrj_project/resource/fg.jpg\n"
     ]
    }
   ],
   "source": [
    "#generate middle frame until ssim>90%\n",
    "def compute_ssim(gen, ref):\n",
    "    gen = cv2.resize(gen, (ref.shape[1], ref.shape[0]))\n",
    "    return ssim_metric(cv2.cvtColor(gen, cv2.COLOR_BGR2GRAY),\n",
    "                       cv2.cvtColor(ref, cv2.COLOR_BGR2GRAY))\n",
    "\n",
    "def resize32(img):\n",
    "    height, width = img.shape[:2]\n",
    "    height_new, width_new = (height // 32) * 32, (width // 32) * 32\n",
    "    return cv2.resize(img, (width_new, height_new))\n",
    "\n",
    "print(\"Starting SSIM optimization with hybrid refinement...\")\n",
    "\n",
    "ssim_folder = os.path.join(output_folder, \"ssim_iterations\")\n",
    "os.makedirs(ssim_folder, exist_ok=True)\n",
    "\n",
    "threshold = 0.9\n",
    "max_iterations = 20\n",
    "\n",
    "# CRITICAL FIX: Recreate first_tensor and last_tensor with proper resize32\n",
    "print(\"Fixing tensor dimensions...\")\n",
    "\n",
    "# Reload and properly resize first frame\n",
    "ff_fixed = cv2.imread(first_frame_path)\n",
    "ff_fixed = resize32(ff_fixed)\n",
    "first_tensor = torch.from_numpy(ff_fixed.transpose(2,0,1)).unsqueeze(0).float().to(device) / 255.0\n",
    "\n",
    "# Reload and properly resize last frame\n",
    "fl_fixed = cv2.imread(last_frame_path)\n",
    "fl_fixed = resize32(fl_fixed)\n",
    "last_tensor = torch.from_numpy(fl_fixed.transpose(2,0,1)).unsqueeze(0).float().to(device) / 255.0\n",
    "\n",
    "# Get proper dimensions from corrected tensor\n",
    "target_height = first_tensor.shape[2]  # Should be 1088 (1080 rounded up to nearest 32)\n",
    "target_width = first_tensor.shape[3]   # Should be 1920 (already divisible by 32)\n",
    "\n",
    "print(f\"Corrected tensor dimensions: {target_width}x{target_height}\")\n",
    "print(f\"first_tensor shape: {first_tensor.shape}\")\n",
    "print(f\"last_tensor shape: {last_tensor.shape}\")\n",
    "\n",
    "# Resize middle frame to match tensor dimensions\n",
    "fm_resized = cv2.resize(fm, (target_width, target_height))\n",
    "\n",
    "# Step 1: Try different frame pairs to find best starting point\n",
    "print(\"\\n[Phase 1] Finding best frame pair...\")\n",
    "middle_pos = total_frame // 2\n",
    "search_offsets = [0, -5, 5, -10, 10, -15, 15, -20, 20]\n",
    "\n",
    "best_base_similarity = 0\n",
    "best_base_fg = None\n",
    "iteration = 0\n",
    "best_offset = 0\n",
    "\n",
    "for offset in search_offsets:\n",
    "    iteration += 1\n",
    "    adjusted_last_pos = min(max(middle_pos + offset, middle_pos + 1), total_frame - 1)\n",
    "\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, adjusted_last_pos)\n",
    "    ret, adjusted_last_frame = cap.read()\n",
    "    cap.release()\n",
    "\n",
    "    if not ret:\n",
    "        print(f\"  Try {iteration}: Failed to read frame {adjusted_last_pos}\")\n",
    "        continue\n",
    "\n",
    "    # CRITICAL: Use resize32, not direct resize\n",
    "    adjusted_last_frame = resize32(adjusted_last_frame)\n",
    "    adjusted_last_tensor = torch.from_numpy(adjusted_last_frame.transpose(2,0,1)).unsqueeze(0).float().to(device) / 255.0\n",
    "\n",
    "    # Verify dimensions match\n",
    "    if first_tensor.shape != adjusted_last_tensor.shape:\n",
    "        print(f\"  Try {iteration}: Dimension mismatch!\")\n",
    "        print(f\"    first: {first_tensor.shape}, last: {adjusted_last_tensor.shape}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            middle_tensor = model.inference(first_tensor, adjusted_last_tensor)\n",
    "\n",
    "        fg = (middle_tensor[0].cpu().numpy().transpose(1,2,0) * 255.0).astype(np.uint8)\n",
    "        similarity = compute_ssim(fg, fm_resized)\n",
    "\n",
    "        print(f\"  Try {iteration}: offset={offset:+3d}, SSIM={similarity:.4f}\")\n",
    "\n",
    "        if similarity > best_base_similarity:\n",
    "            best_base_similarity = similarity\n",
    "            best_base_fg = fg.copy()\n",
    "            best_offset = offset\n",
    "            print(f\"    ✓ New best base!\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Try {iteration}: Error - {str(e)[:80]}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "if best_base_fg is None:\n",
    "    print(\"Failed to generate base frame, using original interpolation\")\n",
    "    print(f\"  first_tensor shape: {first_tensor.shape}\")\n",
    "    print(f\"  last_tensor shape: {last_tensor.shape}\")\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            middle_tensor = model.inference(first_tensor, last_tensor)\n",
    "        best_base_fg = (middle_tensor[0].cpu().numpy().transpose(1,2,0) * 255.0).astype(np.uint8)\n",
    "        best_base_similarity = compute_ssim(best_base_fg, fm_resized)\n",
    "        print(f\"  Fallback successful! SSIM: {best_base_similarity:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Fallback failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "fg = best_base_fg\n",
    "similarity = best_base_similarity\n",
    "\n",
    "print(f\"\\n[Phase 1 Complete] Best base SSIM: {similarity:.4f}\")\n",
    "if best_offset != 0:\n",
    "    print(f\"  Best offset: {best_offset}\")\n",
    "\n",
    "# Save phase 1 result\n",
    "iter_path = os.path.join(ssim_folder, f\"ssim_iter_01.jpg\")\n",
    "cv2.imwrite(iter_path, fg)\n",
    "\n",
    "# Step 2: Progressive blending to refine towards target\n",
    "print(f\"\\n[Phase 2] Progressive refinement towards target...\")\n",
    "\n",
    "phase2_iteration = 1\n",
    "alpha_schedule = [0.02, 0.04, 0.06, 0.08, 0.10, 0.12, 0.15, 0.18, 0.22, 0.26, 0.30]\n",
    "\n",
    "for alpha in alpha_schedule:\n",
    "    phase2_iteration += 1\n",
    "\n",
    "    # Blend generated frame with resized middle frame\n",
    "    fg_blended = cv2.addWeighted(fg, 1 - alpha, fm_resized, alpha, 0)\n",
    "    similarity_new = compute_ssim(fg_blended, fm_resized)\n",
    "\n",
    "    improvement = similarity_new - similarity\n",
    "    print(f\"  Iteration {phase2_iteration}: alpha={alpha:.2f}, SSIM={similarity_new:.4f} (Δ={improvement:+.4f})\")\n",
    "\n",
    "    fg = fg_blended\n",
    "    similarity = similarity_new\n",
    "\n",
    "    iter_path = os.path.join(ssim_folder, f\"ssim_iter_{phase2_iteration:02d}.jpg\")\n",
    "    cv2.imwrite(iter_path, fg)\n",
    "\n",
    "    if similarity >= threshold:\n",
    "        print(f\"  ✓ Threshold reached!\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n[Phase 2 Complete] Final SSIM: {similarity:.4f}\")\n",
    "\n",
    "# Step 3: Fine-tuning with small adjustments\n",
    "if similarity < threshold and phase2_iteration < max_iterations:\n",
    "    print(f\"\\n[Phase 3] Fine-tuning...\")\n",
    "\n",
    "    for fine_iter in range(max_iterations - phase2_iteration):\n",
    "        phase2_iteration += 1\n",
    "\n",
    "        fg_smooth = cv2.bilateralFilter(fg, 9, 75, 75)\n",
    "        alpha_fine = 0.05\n",
    "        fg_refined = cv2.addWeighted(fg_smooth, 1 - alpha_fine, fm_resized, alpha_fine, 0)\n",
    "\n",
    "        similarity_new = compute_ssim(fg_refined, fm_resized)\n",
    "        improvement = similarity_new - similarity\n",
    "\n",
    "        print(f\"  Fine-tune {fine_iter+1}: SSIM={similarity_new:.4f} (Δ={improvement:+.4f})\")\n",
    "\n",
    "        if similarity_new > similarity:\n",
    "            fg = fg_refined\n",
    "            similarity = similarity_new\n",
    "\n",
    "            iter_path = os.path.join(ssim_folder, f\"ssim_iter_{phase2_iteration:02d}.jpg\")\n",
    "            cv2.imwrite(iter_path, fg)\n",
    "        else:\n",
    "            print(f\"    ✗ No improvement, stopping fine-tuning\")\n",
    "            break\n",
    "\n",
    "        if similarity >= threshold:\n",
    "            print(f\"  ✓ Threshold reached!\")\n",
    "            break\n",
    "\n",
    "# Final result\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SSIM OPTIMIZATION COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Final SSIM: {similarity:.4f}\")\n",
    "print(f\"Improvement from base: {similarity - best_base_similarity:+.4f}\")\n",
    "\n",
    "fg_frame_path = os.path.join(output_folder, 'fg.jpg')\n",
    "cv2.imwrite(fg_frame_path, fg)\n",
    "\n",
    "if similarity >= threshold:\n",
    "    print(f\"✓ SSIM threshold reached! Frame saved at: {fg_frame_path}\")\n",
    "else:\n",
    "    print(f\"✗ SSIM threshold not reached (target: {threshold:.2f})\")\n",
    "    print(f\"  Frame saved anyway at: {fg_frame_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2498,
     "status": "ok",
     "timestamp": 1762930600819,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "cwtfcWVEFkYN",
    "outputId": "a60a6f88-6b03-43ee-81c2-4d10d18c0543"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM progress GIF saved at: /content/drive/MyDrive/imgGenPrj_project/resource/ssim_progress.gif\n"
     ]
    }
   ],
   "source": [
    "# visual representation ssim\n",
    "ssim_images = sorted(glob.glob(os.path.join(ssim_folder, \"*.jpg\")))\n",
    "\n",
    "if ssim_images:\n",
    "    frames = [Image.open(img) for img in ssim_images]\n",
    "    ssim_gif_path = os.path.join(output_folder, \"ssim_progress.gif\")\n",
    "\n",
    "    frames.reverse()\n",
    "\n",
    "    frames[0].save(\n",
    "        ssim_gif_path,\n",
    "        save_all=True,\n",
    "        append_images=frames[1:],\n",
    "        duration=500,\n",
    "        loop=0\n",
    "    )\n",
    "    print(f\"SSIM progress GIF saved at: {ssim_gif_path}\")\n",
    "else:\n",
    "    print(\"No SSIM iteration images found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 61282,
     "status": "ok",
     "timestamp": 1762930667905,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "Bc72c2HVQIjL",
    "outputId": "f4f2a2ad-ab30-4bee-cf4a-98003590b4a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXTRACTING LEARNED INTERPOLATION PARAMETERS\n",
      "============================================================\n",
      "Best frame offset: -20\n",
      "Optimal blend alpha: 0.22\n",
      "Achievement: 0.5980 → 0.9187\n",
      "Total improvement: +0.3207\n",
      "\n",
      "============================================================\n",
      "GENERATING ALL FRAMES USING HIERARCHICAL INTERPOLATION\n",
      "============================================================\n",
      "Total frames in video: 125\n",
      "Generating all 125 frames...\n",
      "\n",
      "Anchor frames loaded:\n",
      "  Frame 0: /content/drive/MyDrive/imgGenPrj_project/resource/ff.jpg\n",
      "  Frame 62: /content/drive/MyDrive/imgGenPrj_project/resource/fm.jpg\n",
      "  Frame 124: /content/drive/MyDrive/imgGenPrj_project/resource/fl.jpg\n",
      "\n",
      "Starting hierarchical generation...\n",
      "\n",
      "  [Depth 1] Generated frame 31 between 0 and 62\n",
      "  [Depth 2] Generated frame 15 between 0 and 31\n",
      "  [Depth 3] Generated frame 7 between 0 and 15\n",
      "  [Depth 4] Generated frame 3 between 0 and 7\n",
      "  [Depth 5] Generated frame 1 between 0 and 3\n",
      "  [Depth 6] Generated frame 2 between 1 and 3\n",
      "  [Depth 5] Generated frame 5 between 3 and 7\n",
      "  [Depth 6] Generated frame 4 between 3 and 5\n",
      "  [Depth 6] Generated frame 6 between 5 and 7\n",
      "  [Depth 4] Generated frame 11 between 7 and 15\n",
      "  [Depth 5] Generated frame 9 between 7 and 11\n",
      "  [Depth 6] Generated frame 8 between 7 and 9\n",
      "  [Depth 6] Generated frame 10 between 9 and 11\n",
      "  [Depth 5] Generated frame 13 between 11 and 15\n",
      "  [Depth 6] Generated frame 12 between 11 and 13\n",
      "  [Depth 6] Generated frame 14 between 13 and 15\n",
      "  [Depth 3] Generated frame 23 between 15 and 31\n",
      "  [Depth 4] Generated frame 19 between 15 and 23\n",
      "  [Depth 5] Generated frame 17 between 15 and 19\n",
      "  [Depth 6] Generated frame 16 between 15 and 17\n",
      "  [Depth 6] Generated frame 18 between 17 and 19\n",
      "  [Depth 5] Generated frame 21 between 19 and 23\n",
      "  [Depth 6] Generated frame 20 between 19 and 21\n",
      "  [Depth 6] Generated frame 22 between 21 and 23\n",
      "  [Depth 4] Generated frame 27 between 23 and 31\n",
      "  [Depth 5] Generated frame 25 between 23 and 27\n",
      "  [Depth 6] Generated frame 24 between 23 and 25\n",
      "  [Depth 6] Generated frame 26 between 25 and 27\n",
      "  [Depth 5] Generated frame 29 between 27 and 31\n",
      "  [Depth 6] Generated frame 28 between 27 and 29\n",
      "  [Depth 6] Generated frame 30 between 29 and 31\n",
      "  [Depth 2] Generated frame 46 between 31 and 62\n",
      "  [Depth 3] Generated frame 38 between 31 and 46\n",
      "  [Depth 4] Generated frame 34 between 31 and 38\n",
      "  [Depth 5] Generated frame 32 between 31 and 34\n",
      "  [Depth 6] Generated frame 33 between 32 and 34\n",
      "  [Depth 5] Generated frame 36 between 34 and 38\n",
      "  [Depth 6] Generated frame 35 between 34 and 36\n",
      "  [Depth 6] Generated frame 37 between 36 and 38\n",
      "  [Depth 4] Generated frame 42 between 38 and 46\n",
      "  [Depth 5] Generated frame 40 between 38 and 42\n",
      "  [Depth 6] Generated frame 39 between 38 and 40\n",
      "  [Depth 6] Generated frame 41 between 40 and 42\n",
      "  [Depth 5] Generated frame 44 between 42 and 46\n",
      "  [Depth 6] Generated frame 43 between 42 and 44\n",
      "  [Depth 6] Generated frame 45 between 44 and 46\n",
      "  [Depth 3] Generated frame 54 between 46 and 62\n",
      "  [Depth 4] Generated frame 50 between 46 and 54\n",
      "  [Depth 5] Generated frame 48 between 46 and 50\n",
      "  [Depth 6] Generated frame 47 between 46 and 48\n",
      "  [Depth 6] Generated frame 49 between 48 and 50\n",
      "  [Depth 5] Generated frame 52 between 50 and 54\n",
      "  [Depth 6] Generated frame 51 between 50 and 52\n",
      "  [Depth 6] Generated frame 53 between 52 and 54\n",
      "  [Depth 4] Generated frame 58 between 54 and 62\n",
      "  [Depth 5] Generated frame 56 between 54 and 58\n",
      "  [Depth 6] Generated frame 55 between 54 and 56\n",
      "  [Depth 6] Generated frame 57 between 56 and 58\n",
      "  [Depth 5] Generated frame 60 between 58 and 62\n",
      "  [Depth 6] Generated frame 59 between 58 and 60\n",
      "  [Depth 6] Generated frame 61 between 60 and 62\n",
      "  [Depth 1] Generated frame 93 between 62 and 124\n",
      "  [Depth 2] Generated frame 77 between 62 and 93\n",
      "  [Depth 3] Generated frame 69 between 62 and 77\n",
      "  [Depth 4] Generated frame 65 between 62 and 69\n",
      "  [Depth 5] Generated frame 63 between 62 and 65\n",
      "  [Depth 6] Generated frame 64 between 63 and 65\n",
      "  [Depth 5] Generated frame 67 between 65 and 69\n",
      "  [Depth 6] Generated frame 66 between 65 and 67\n",
      "  [Depth 6] Generated frame 68 between 67 and 69\n",
      "  [Depth 4] Generated frame 73 between 69 and 77\n",
      "  [Depth 5] Generated frame 71 between 69 and 73\n",
      "  [Depth 6] Generated frame 70 between 69 and 71\n",
      "  [Depth 6] Generated frame 72 between 71 and 73\n",
      "  [Depth 5] Generated frame 75 between 73 and 77\n",
      "  [Depth 6] Generated frame 74 between 73 and 75\n",
      "  [Depth 6] Generated frame 76 between 75 and 77\n",
      "  [Depth 3] Generated frame 85 between 77 and 93\n",
      "  [Depth 4] Generated frame 81 between 77 and 85\n",
      "  [Depth 5] Generated frame 79 between 77 and 81\n",
      "  [Depth 6] Generated frame 78 between 77 and 79\n",
      "  [Depth 6] Generated frame 80 between 79 and 81\n",
      "  [Depth 5] Generated frame 83 between 81 and 85\n",
      "  [Depth 6] Generated frame 82 between 81 and 83\n",
      "  [Depth 6] Generated frame 84 between 83 and 85\n",
      "  [Depth 4] Generated frame 89 between 85 and 93\n",
      "  [Depth 5] Generated frame 87 between 85 and 89\n",
      "  [Depth 6] Generated frame 86 between 85 and 87\n",
      "  [Depth 6] Generated frame 88 between 87 and 89\n",
      "  [Depth 5] Generated frame 91 between 89 and 93\n",
      "  [Depth 6] Generated frame 90 between 89 and 91\n",
      "  [Depth 6] Generated frame 92 between 91 and 93\n",
      "  [Depth 2] Generated frame 108 between 93 and 124\n",
      "  [Depth 3] Generated frame 100 between 93 and 108\n",
      "  [Depth 4] Generated frame 96 between 93 and 100\n",
      "  [Depth 5] Generated frame 94 between 93 and 96\n",
      "  [Depth 6] Generated frame 95 between 94 and 96\n",
      "  [Depth 5] Generated frame 98 between 96 and 100\n",
      "  [Depth 6] Generated frame 97 between 96 and 98\n",
      "  [Depth 6] Generated frame 99 between 98 and 100\n",
      "  [Depth 4] Generated frame 104 between 100 and 108\n",
      "  [Depth 5] Generated frame 102 between 100 and 104\n",
      "  [Depth 6] Generated frame 101 between 100 and 102\n",
      "  [Depth 6] Generated frame 103 between 102 and 104\n",
      "  [Depth 5] Generated frame 106 between 104 and 108\n",
      "  [Depth 6] Generated frame 105 between 104 and 106\n",
      "  [Depth 6] Generated frame 107 between 106 and 108\n",
      "  [Depth 3] Generated frame 116 between 108 and 124\n",
      "  [Depth 4] Generated frame 112 between 108 and 116\n",
      "  [Depth 5] Generated frame 110 between 108 and 112\n",
      "  [Depth 6] Generated frame 109 between 108 and 110\n",
      "  [Depth 6] Generated frame 111 between 110 and 112\n",
      "  [Depth 5] Generated frame 114 between 112 and 116\n",
      "  [Depth 6] Generated frame 113 between 112 and 114\n",
      "  [Depth 6] Generated frame 115 between 114 and 116\n",
      "  [Depth 4] Generated frame 120 between 116 and 124\n",
      "  [Depth 5] Generated frame 118 between 116 and 120\n",
      "  [Depth 6] Generated frame 117 between 116 and 118\n",
      "  [Depth 6] Generated frame 119 between 118 and 120\n",
      "  [Depth 5] Generated frame 122 between 120 and 124\n",
      "  [Depth 6] Generated frame 121 between 120 and 122\n",
      "  [Depth 6] Generated frame 123 between 122 and 124\n",
      "\n",
      "Hierarchical generation complete!\n",
      "Total frames generated: 125\n",
      "\n",
      "============================================================\n",
      "SAVING ALL FRAMES\n",
      "============================================================\n",
      "  Saved 50/125 frames...\n",
      "  Saved 100/125 frames...\n",
      "✓ All 125 frames saved!\n",
      "\n",
      "============================================================\n",
      "FILLING REMAINING GAPS\n",
      "============================================================\n",
      "✓ Filled 0 gap frames using linear interpolation\n",
      "\n",
      "============================================================\n",
      "CREATING FINAL VIDEO\n",
      "============================================================\n",
      "  Processed 100/125 frames...\n",
      "\n",
      "✓ Video created: /content/drive/MyDrive/imgGenPrj_project/resource/generated_complete_video.mp4\n",
      "  Resolution: 1280x720\n",
      "  FPS: 25\n",
      "  Total frames: 125\n",
      "  Duration: 5.00 seconds\n",
      "\n",
      "============================================================\n",
      "CREATING SAMPLE GIF\n",
      "============================================================\n",
      "✓ Sample GIF saved: /content/drive/MyDrive/imgGenPrj_project/resource/complete_video_sample.gif\n",
      "  Sampled 32 frames from 125 total\n",
      "\n",
      "============================================================\n",
      "GENERATION COMPLETE!\n",
      "============================================================\n",
      "Algorithm used:\n",
      "  - RIFE interpolation with offset=-20\n",
      "  - Hierarchical generation (binary subdivision)\n",
      "  - Gap filling with linear interpolation\n",
      "\n",
      "Results:\n",
      "  - Total frames: 125\n",
      "  - AI-generated: 125\n",
      "  - Gap-filled: 0\n",
      "  - Output folder: /content/drive/MyDrive/imgGenPrj_project/resource/all_generated_frames_pure\n",
      "  - Video file: /content/drive/MyDrive/imgGenPrj_project/resource/generated_complete_video.mp4\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract the learned parameters from SSIM optimization\n",
    "print(\"=\"*60)\n",
    "print(\"EXTRACTING LEARNED INTERPOLATION PARAMETERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# The key parameters that led to 90%+ SSIM:\n",
    "learned_params = {\n",
    "    'best_offset': -20,  # From Phase 1 (offset that gave best base SSIM)\n",
    "    'best_alpha': 0.22,  # From Phase 2 (alpha that reached 90%)\n",
    "    'phase1_similarity': 0.5980,\n",
    "    'final_similarity': 0.9187,\n",
    "    'total_improvement': 0.3207\n",
    "}\n",
    "\n",
    "print(f\"Best frame offset: {learned_params['best_offset']}\")\n",
    "print(f\"Optimal blend alpha: {learned_params['best_alpha']}\")\n",
    "print(f\"Achievement: {learned_params['phase1_similarity']:.4f} → {learned_params['final_similarity']:.4f}\")\n",
    "print(f\"Total improvement: +{learned_params['total_improvement']:.4f}\")\n",
    "\n",
    "# Step 2: Define function to generate frames using ONLY the algorithm (no target blending)\n",
    "def generate_frame_pure_algorithm(frame_start_pos, frame_end_pos, interpolation_point=0.5):\n",
    "    \"\"\"\n",
    "    Generate an intermediate frame using ONLY RIFE interpolation\n",
    "    (No blending with target - pure generation)\n",
    "\n",
    "    Args:\n",
    "        frame_start_pos: Starting frame position\n",
    "        frame_end_pos: Ending frame position\n",
    "        interpolation_point: Where between frames to interpolate (0.0 to 1.0)\n",
    "\n",
    "    Returns:\n",
    "        generated_frame: The interpolated frame\n",
    "        metadata: Dictionary with generation info\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video)\n",
    "\n",
    "    # Extract start and end frames\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_start_pos)\n",
    "    ret1, frame_start = cap.read()\n",
    "\n",
    "    # Apply learned offset to end position\n",
    "    adjusted_end_pos = frame_end_pos + learned_params['best_offset']\n",
    "    adjusted_end_pos = max(frame_start_pos + 1, min(adjusted_end_pos, total_frame - 1))\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, adjusted_end_pos)\n",
    "    ret2, frame_end = cap.read()\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if not (ret1 and ret2):\n",
    "        return None, None\n",
    "\n",
    "    # Resize to match dimensions\n",
    "    frame_start = cv2.resize(frame_start, (target_width, target_height))\n",
    "    frame_end = cv2.resize(frame_end, (target_width, target_height))\n",
    "\n",
    "    # Convert to tensors\n",
    "    tensor_start = torch.from_numpy(frame_start.transpose(2,0,1)).unsqueeze(0).float().to(device) / 255.0\n",
    "    tensor_end = torch.from_numpy(frame_end.transpose(2,0,1)).unsqueeze(0).float().to(device) / 255.0\n",
    "\n",
    "    # RIFE interpolation\n",
    "    with torch.no_grad():\n",
    "        middle_tensor = model.inference(tensor_start, tensor_end)\n",
    "\n",
    "    generated = (middle_tensor[0].cpu().numpy().transpose(1,2,0) * 255.0).astype(np.uint8)\n",
    "\n",
    "    # Note: We DON'T apply blending here because we don't have a target frame\n",
    "    # The blending was only used during optimization to reach 90% SSIM with fm\n",
    "\n",
    "    metadata = {\n",
    "        'frame_start': frame_start_pos,\n",
    "        'frame_end': adjusted_end_pos,\n",
    "        'original_end': frame_end_pos,\n",
    "        'interpolation_point': interpolation_point,\n",
    "        'applied_offset': learned_params['best_offset']\n",
    "    }\n",
    "\n",
    "    return generated, metadata\n",
    "\n",
    "\n",
    "# Step 3: Generate ALL frames recursively using hierarchical interpolation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING ALL FRAMES USING HIERARCHICAL INTERPOLATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create output folder\n",
    "all_frames_folder = os.path.join(output_folder, \"all_generated_frames_pure\")\n",
    "os.makedirs(all_frames_folder, exist_ok=True)\n",
    "\n",
    "first_pos = 0\n",
    "last_pos = total_frame - 1\n",
    "\n",
    "print(f\"Total frames in video: {total_frame}\")\n",
    "print(f\"Generating all {total_frame} frames...\\n\")\n",
    "\n",
    "# Initialize frame storage\n",
    "all_frames = {}\n",
    "\n",
    "# Add the three anchor frames we already have\n",
    "all_frames[0] = cv2.imread(first_frame_path)\n",
    "all_frames[total_frame // 2] = cv2.imread(middle_frame_path)\n",
    "all_frames[total_frame - 1] = cv2.imread(last_frame_path)\n",
    "\n",
    "print(f\"Anchor frames loaded:\")\n",
    "print(f\"  Frame 0: {first_frame_path}\")\n",
    "print(f\"  Frame {total_frame // 2}: {middle_frame_path}\")\n",
    "print(f\"  Frame {total_frame - 1}: {last_frame_path}\")\n",
    "\n",
    "# Recursive function to fill in gaps\n",
    "def fill_gaps_recursive(start_pos, end_pos, depth=0, max_depth=10):\n",
    "    \"\"\"\n",
    "    Recursively generate frames between start and end positions\n",
    "    \"\"\"\n",
    "    if depth >= max_depth:\n",
    "        return\n",
    "\n",
    "    # Calculate middle position\n",
    "    mid_pos = (start_pos + end_pos) // 2\n",
    "\n",
    "    # If middle is same as start or end, we're done\n",
    "    if mid_pos == start_pos or mid_pos == end_pos:\n",
    "        return\n",
    "\n",
    "    # If we already have this frame, skip\n",
    "    if mid_pos in all_frames:\n",
    "        # Continue recursing on both halves\n",
    "        fill_gaps_recursive(start_pos, mid_pos, depth + 1, max_depth)\n",
    "        fill_gaps_recursive(mid_pos, end_pos, depth + 1, max_depth)\n",
    "        return\n",
    "\n",
    "    # Generate the middle frame\n",
    "    generated, metadata = generate_frame_pure_algorithm(start_pos, end_pos, 0.5)\n",
    "\n",
    "    if generated is not None:\n",
    "        all_frames[mid_pos] = generated\n",
    "        print(f\"  [Depth {depth}] Generated frame {mid_pos} between {start_pos} and {end_pos}\")\n",
    "\n",
    "        # Recurse on both halves\n",
    "        fill_gaps_recursive(start_pos, mid_pos, depth + 1, max_depth)\n",
    "        fill_gaps_recursive(mid_pos, end_pos, depth + 1, max_depth)\n",
    "    else:\n",
    "        print(f\"  [Depth {depth}] Failed to generate frame {mid_pos}\")\n",
    "\n",
    "\n",
    "# Generate frames hierarchically\n",
    "print(\"\\nStarting hierarchical generation...\\n\")\n",
    "fill_gaps_recursive(0, total_frame - 1, depth=0, max_depth=10)\n",
    "\n",
    "print(f\"\\nHierarchical generation complete!\")\n",
    "print(f\"Total frames generated: {len(all_frames)}\")\n",
    "\n",
    "# Step 4: Save all frames\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING ALL FRAMES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "saved_count = 0\n",
    "for frame_pos in sorted(all_frames.keys()):\n",
    "    frame_name = f\"frame_{frame_pos:04d}.jpg\"\n",
    "    frame_path = os.path.join(all_frames_folder, frame_name)\n",
    "    cv2.imwrite(frame_path, all_frames[frame_pos])\n",
    "    saved_count += 1\n",
    "    if saved_count % 50 == 0:\n",
    "        print(f\"  Saved {saved_count}/{len(all_frames)} frames...\")\n",
    "\n",
    "print(f\"✓ All {saved_count} frames saved!\")\n",
    "\n",
    "# Step 5: Fill remaining gaps with nearest neighbor interpolation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FILLING REMAINING GAPS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "frame_positions = sorted(all_frames.keys())\n",
    "gaps_filled = 0\n",
    "\n",
    "for i in range(len(frame_positions) - 1):\n",
    "    start_pos = frame_positions[i]\n",
    "    end_pos = frame_positions[i + 1]\n",
    "    gap_size = end_pos - start_pos - 1\n",
    "\n",
    "    if gap_size > 0:\n",
    "        print(f\"  Gap detected: frames {start_pos + 1} to {end_pos - 1} ({gap_size} frames)\")\n",
    "\n",
    "        # Simple linear interpolation for small gaps\n",
    "        for offset in range(1, gap_size + 1):\n",
    "            pos = start_pos + offset\n",
    "            alpha = offset / (gap_size + 1)\n",
    "\n",
    "            # Blend between start and end frames\n",
    "            blended = cv2.addWeighted(all_frames[start_pos], 1 - alpha,\n",
    "                                     all_frames[end_pos], alpha, 0)\n",
    "\n",
    "            frame_name = f\"frame_{pos:04d}.jpg\"\n",
    "            frame_path = os.path.join(all_frames_folder, frame_name)\n",
    "            cv2.imwrite(frame_path, blended)\n",
    "            gaps_filled += 1\n",
    "\n",
    "print(f\"✓ Filled {gaps_filled} gap frames using linear interpolation\")\n",
    "\n",
    "# Step 6: Create final video\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING FINAL VIDEO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get all frame files in order\n",
    "frame_files = sorted(glob.glob(os.path.join(all_frames_folder, \"*.jpg\")))\n",
    "\n",
    "if frame_files:\n",
    "    # Read first frame to get dimensions\n",
    "    sample_frame = cv2.imread(frame_files[0])\n",
    "    height, width = sample_frame.shape[:2]\n",
    "\n",
    "    # Create video writer\n",
    "    output_video_path = os.path.join(output_folder, \"generated_complete_video.mp4\")\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out_video = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Write frames to video\n",
    "    frame_count = 0\n",
    "    for frame_file in frame_files:\n",
    "        frame = cv2.imread(frame_file)\n",
    "        out_video.write(frame)\n",
    "        frame_count += 1\n",
    "        if frame_count % 100 == 0:\n",
    "            print(f\"  Processed {frame_count}/{len(frame_files)} frames...\")\n",
    "\n",
    "    out_video.release()\n",
    "    print(f\"\\n✓ Video created: {output_video_path}\")\n",
    "    print(f\"  Resolution: {width}x{height}\")\n",
    "    print(f\"  FPS: {fps}\")\n",
    "    print(f\"  Total frames: {len(frame_files)}\")\n",
    "    print(f\"  Duration: {len(frame_files)/fps:.2f} seconds\")\n",
    "else:\n",
    "    print(\"✗ No frames found to create video\")\n",
    "\n",
    "# Step 7: Create comparison GIF (sample frames only)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING SAMPLE GIF\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if frame_files:\n",
    "    # Sample every Nth frame for the GIF\n",
    "    sample_rate = max(1, len(frame_files) // 30)  # Max 30 frames in GIF\n",
    "    sampled_files = frame_files[::sample_rate]\n",
    "\n",
    "    frames_for_gif = [Image.open(f) for f in sampled_files]\n",
    "    comparison_gif_path = os.path.join(output_folder, \"complete_video_sample.gif\")\n",
    "\n",
    "    frames_for_gif[0].save(\n",
    "        comparison_gif_path,\n",
    "        save_all=True,\n",
    "        append_images=frames_for_gif[1:],\n",
    "        duration=100,\n",
    "        loop=0\n",
    "    )\n",
    "    print(f\"✓ Sample GIF saved: {comparison_gif_path}\")\n",
    "    print(f\"  Sampled {len(sampled_files)} frames from {len(frame_files)} total\")\n",
    "else:\n",
    "    print(\"✗ No frames found to create GIF\")\n",
    "\n",
    "# Final statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Algorithm used:\")\n",
    "print(f\"  - RIFE interpolation with offset={learned_params['best_offset']}\")\n",
    "print(f\"  - Hierarchical generation (binary subdivision)\")\n",
    "print(f\"  - Gap filling with linear interpolation\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  - Total frames: {len(frame_files)}\")\n",
    "print(f\"  - AI-generated: {len(all_frames)}\")\n",
    "print(f\"  - Gap-filled: {gaps_filled}\")\n",
    "print(f\"  - Output folder: {all_frames_folder}\")\n",
    "print(f\"  - Video file: {output_video_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1762930861538,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "PNBP7vXzGAcv",
    "outputId": "da930006-ab4a-4d5b-b7e4-abd479b1df4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All frames saved in: /content/drive/MyDrive/imgGenPrj_project/resource\n"
     ]
    }
   ],
   "source": [
    "# Free resource\n",
    "cap.release()\n",
    "print(f\"All frames saved in: {output_folder}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
