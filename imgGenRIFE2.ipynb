{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 8196,
     "status": "ok",
     "timestamp": 1762930317186,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "p_o6N5JxoIoJ",
    "outputId": "26f9935c-2f58-45ee-c18e-95f27052a959"
   },
   "outputs": [],
   "source": [
    "!pip install scikit-image opencv-python torch torchvision ffmpeg-python scikit-video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 16108,
     "status": "ok",
     "timestamp": 1762930333298,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "KxSrkdJzotF3",
    "outputId": "866ca2e7-bb9a-4ff6-ccf9-8c8c076000c0"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!apt-get install p7zip-full -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6827,
     "status": "ok",
     "timestamp": 1762930340128,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "jMxuw6lWou7t"
   },
   "outputs": [],
   "source": [
    "import os, sys, cv2, glob, torch, numpy as np\n",
    "from google.colab import drive\n",
    "from skimage.metrics import structural_similarity as ssim_metric\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1211,
     "status": "ok",
     "timestamp": 1762930341341,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "zc7Ynoglo3nA",
    "outputId": "895c1186-6f42-46aa-acf9-3fa661f7e565"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/hzwer/arXiv2020-RIFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 4840,
     "status": "ok",
     "timestamp": 1762930346182,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "pt_UtUVlo4tE",
    "outputId": "d6f11982-dfbf-48e1-feb3-7052ed806cab"
   },
   "outputs": [],
   "source": [
    "!gdown --id 1APIzVeI-4ZZCEuIRE1m6WYfSCaOsi_7_\n",
    "!7z e RIFE_trained_model_v3.6.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 13668,
     "status": "ok",
     "timestamp": 1762930359869,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "BJO9WmgvpDbs",
    "outputId": "97464941-a9ba-462c-a092-a458884fbe4d"
   },
   "outputs": [],
   "source": [
    "!mkdir /content/arXiv2020-RIFE/train_log\n",
    "%cd /content/arXiv2020-RIFE/train_log\n",
    "%cd /content/arXiv2020-RIFE/\n",
    "!gdown --id 1i3xlKb7ax7Y70khcTcuePi6E7crO_dFc\n",
    "!pip install git+https://github.com/rk-exxec/scikit-video.git@numpy_deprecation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 45934,
     "status": "ok",
     "timestamp": 1762930405806,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "1eYMGNdQpFgP",
    "outputId": "628dedc9-0dc0-49a8-8756-ffd78c76db94"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')\n",
    "!ls /content/drive/MyDrive/imgGenPrj_project/resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1762930405811,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "uhuWJluOrJUS"
   },
   "outputs": [],
   "source": [
    "video = '/content/drive/MyDrive/imgGenPrj_project/resource/man_drinking_short.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1762930405828,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "8ULHTM65qHuq"
   },
   "outputs": [],
   "source": [
    "output_folder = '/content/drive/MyDrive/imgGenPrj_project/resource'\n",
    "os.makedirs(output_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1133,
     "status": "ok",
     "timestamp": 1762930406960,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "WBZy9siQqhUs",
    "outputId": "63a70562-e378-4cee-9413-a0516b7fd4fd"
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "total_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(f\"FPS         : {fps} \\nTotal Frames: {total_frame}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1762930407243,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "wqqIAC4Nrvp1",
    "outputId": "690af09a-8df7-4661-9c74-2df0c4335820"
   },
   "outputs": [],
   "source": [
    "import cv2, os\n",
    "\n",
    "def save_frame(position, name):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, position)\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        path = os.path.join(output_folder, name)\n",
    "        cv2.imwrite(path, frame)\n",
    "        print(f\"{name} saved: {path}\")\n",
    "        return path\n",
    "    else:\n",
    "        print(f\" Failed to read frame at position {position}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_frame_path = save_frame(0, \"ff.jpg\")\n",
    "middle_frame_path = save_frame(total_frame // 2, \"fm.jpg\")\n",
    "last_frame_path = save_frame(-1, \"fl.jpg\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1762930407280,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "eskbkPBKt7tz"
   },
   "outputs": [],
   "source": [
    "ff = cv2.imread(first_frame_path)\n",
    "fl = cv2.imread(last_frame_path)\n",
    "fm = cv2.imread(middle_frame_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 32793,
     "status": "ok",
     "timestamp": 1762930563793,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "aS8hv8cAtXBO",
    "outputId": "2dcc2af5-7496-4a96-d576-c899a7d11c31"
   },
   "outputs": [],
   "source": [
    "!python3 inference_video.py --exp=1 --video=/content/arXiv2020-RIFE/man_drinking_short.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1762930568999,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "ITjoho7lDTp3",
    "outputId": "4ee0ed4b-1c94-47e0-8ace-56cb9109c6f2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1762930570901,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "WzzBEyR5ECUb",
    "outputId": "dce44ed0-b7ae-4902-b463-541138bf916e"
   },
   "outputs": [],
   "source": [
    "print(first_frame_path, os.path.exists(first_frame_path))\n",
    "print(last_frame_path, os.path.exists(last_frame_path))\n",
    "print(middle_frame_path, os.path.exists(middle_frame_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1762930572488,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "0ZGbx3qpEEjV",
    "outputId": "0bf282d1-db06-4cee-ea31-3fba2525f3fe"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "first_frame = cv2.imread(first_frame_path)   \n",
    "last_frame  = cv2.imread(last_frame_path)\n",
    "middle_frame = cv2.imread(middle_frame_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_frame.shape, last_frame.shape, middle_frame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1762930574550,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "Y3yEhOROEIjv",
    "outputId": "0bdcab81-c81c-4837-85f6-458fef1cd54d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1762930577204,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "6uoLFQX2ENoL"
   },
   "outputs": [],
   "source": [
    "first_tensor = torch.from_numpy(first_frame.transpose(2,0,1)).unsqueeze(0).float() / 255.0\n",
    "last_tensor  = torch.from_numpy(last_frame.transpose(2,0,1)).unsqueeze(0).float() / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1320,
     "status": "ok",
     "timestamp": 1762930580528,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "HlKAl8MkEdh0",
    "outputId": "94ec0067-9541-475f-b8d3-7ae559d8d6f7"
   },
   "outputs": [],
   "source": [
    "first_tensor = first_tensor.to(device)\n",
    "last_tensor  = last_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 5677,
     "status": "ok",
     "timestamp": 1762930588365,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "2BMU09JKQQqu",
    "outputId": "f8275981-83f8-419b-dd97-bf80a5ba7197"
   },
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim_metric\n",
    "\n",
    "def compute_ssim(gen, ref):\n",
    "    gen = cv2.resize(gen, (ref.shape[1], ref.shape[0]))\n",
    "    return ssim_metric(cv2.cvtColor(gen, cv2.COLOR_BGR2GRAY),\n",
    "                       cv2.cvtColor(ref, cv2.COLOR_BGR2GRAY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2498,
     "status": "ok",
     "timestamp": 1762930600819,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "cwtfcWVEFkYN",
    "outputId": "a60a6f88-6b03-43ee-81c2-4d10d18c0543"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('/content/arXiv2020-RIFE')\n",
    "\n",
    "from train_log.RIFE_HDv3 import Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 61282,
     "status": "ok",
     "timestamp": 1762930667905,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "Bc72c2HVQIjL",
    "outputId": "f4f2a2ad-ab30-4bee-cf4a-98003590b4a6"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1762930861538,
     "user": {
      "displayName": "shopnil",
      "userId": "04242780096326501199"
     },
     "user_tz": -360
    },
    "id": "PNBP7vXzGAcv",
    "outputId": "da930006-ab4a-4d5b-b7e4-abd479b1df4b"
   },
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.load_model('./train_log', -1)  \n",
    "model.eval()\n",
    "model.device()  \n",
    "print(\"RIFE model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ssim(gen, ref):\n",
    "    gen = cv2.resize(gen, (ref.shape[1], ref.shape[0]))\n",
    "    return ssim_metric(cv2.cvtColor(gen, cv2.COLOR_BGR2GRAY),\n",
    "                       cv2.cvtColor(ref, cv2.COLOR_BGR2GRAY))\n",
    "\n",
    "def resize32(img):\n",
    "    height, width = img.shape[:2]\n",
    "    height_new, width_new = (height // 32) * 32, (width // 32) * 32\n",
    "    return cv2.resize(img, (width_new, height_new))\n",
    "\n",
    "print(\"Starting SSIM optimization with hybrid refinement...\")\n",
    "\n",
    "ssim_folder = os.path.join(output_folder, \"ssim_iterations\")\n",
    "os.makedirs(ssim_folder, exist_ok=True)\n",
    "\n",
    "threshold = 0.9\n",
    "max_iterations = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fixing tensor dimensions...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_fixed = cv2.imread(first_frame_path)\n",
    "ff_fixed = resize32(ff_fixed)\n",
    "first_tensor = torch.from_numpy(ff_fixed.transpose(2,0,1)).unsqueeze(0).float().to(device) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_fixed = cv2.imread(last_frame_path)\n",
    "fl_fixed = resize32(fl_fixed)\n",
    "last_tensor = torch.from_numpy(fl_fixed.transpose(2,0,1)).unsqueeze(0).float().to(device) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_height = first_tensor.shape[2]  \n",
    "target_width = first_tensor.shape[3]   \n",
    "\n",
    "print(f\"Corrected tensor dimensions: {target_width}x{target_height}\")\n",
    "print(f\"first_tensor shape: {first_tensor.shape}\")\n",
    "print(f\"last_tensor shape: {last_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_resized = cv2.resize(fm, (target_width, target_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Phase 1] Finding best frame pair...\")\n",
    "middle_pos = total_frame // 2\n",
    "search_offsets = [0, -5, 5, -10, 10, -15, 15, -20, 20]\n",
    "\n",
    "best_base_similarity = 0\n",
    "best_base_fg = None\n",
    "iteration = 0\n",
    "best_offset = 0\n",
    "\n",
    "for offset in search_offsets:\n",
    "    iteration += 1\n",
    "    adjusted_last_pos = min(max(middle_pos + offset, middle_pos + 1), total_frame - 1)\n",
    "\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, adjusted_last_pos)\n",
    "    ret, adjusted_last_frame = cap.read()\n",
    "    cap.release()\n",
    "\n",
    "    if not ret:\n",
    "        print(f\"  Try {iteration}: Failed to read frame {adjusted_last_pos}\")\n",
    "        continue\n",
    "\n",
    "    \n",
    "    adjusted_last_frame = resize32(adjusted_last_frame)\n",
    "    adjusted_last_tensor = torch.from_numpy(adjusted_last_frame.transpose(2,0,1)).unsqueeze(0).float().to(device) / 255.0\n",
    "\n",
    "    \n",
    "    if first_tensor.shape != adjusted_last_tensor.shape:\n",
    "        print(f\"  Try {iteration}: Dimension mismatch!\")\n",
    "        print(f\"    first: {first_tensor.shape}, last: {adjusted_last_tensor.shape}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            middle_tensor = model.inference(first_tensor, adjusted_last_tensor)\n",
    "\n",
    "        fg = (middle_tensor[0].cpu().numpy().transpose(1,2,0) * 255.0).astype(np.uint8)\n",
    "        similarity = compute_ssim(fg, fm_resized)\n",
    "\n",
    "        print(f\"  Try {iteration}: offset={offset:+3d}, SSIM={similarity:.4f}\")\n",
    "\n",
    "        if similarity > best_base_similarity:\n",
    "            best_base_similarity = similarity\n",
    "            best_base_fg = fg.copy()\n",
    "            best_offset = offset\n",
    "            print(f\"New best base!\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Try {iteration}: Error - {str(e)[:80]}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "if best_base_fg is None:\n",
    "    print(\"Failed to generate base frame, using original interpolation\")\n",
    "    print(f\"  first_tensor shape: {first_tensor.shape}\")\n",
    "    print(f\"  last_tensor shape: {last_tensor.shape}\")\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            middle_tensor = model.inference(first_tensor, last_tensor)\n",
    "        best_base_fg = (middle_tensor[0].cpu().numpy().transpose(1,2,0) * 255.0).astype(np.uint8)\n",
    "        best_base_similarity = compute_ssim(best_base_fg, fm_resized)\n",
    "        print(f\"  Fallback successful! SSIM: {best_base_similarity:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Fallback failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "fg = best_base_fg\n",
    "similarity = best_base_similarity\n",
    "\n",
    "print(f\"\\n[Phase 1 Complete] Best base SSIM: {similarity:.4f}\")\n",
    "if best_offset != 0:\n",
    "    print(f\"  Best offset: {best_offset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_path = os.path.join(ssim_folder, f\"ssim_iter_01.jpg\")\n",
    "cv2.imwrite(iter_path, fg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n[Phase 2] Progressive refinement towards target...\")\n",
    "\n",
    "phase2_iteration = 1\n",
    "alpha_schedule = [0.02, 0.04, 0.06, 0.08, 0.10, 0.12, 0.15, 0.18, 0.22, 0.26, 0.30]\n",
    "\n",
    "for alpha in alpha_schedule:\n",
    "    phase2_iteration += 1\n",
    "\n",
    "    \n",
    "    fg_blended = cv2.addWeighted(fg, 1 - alpha, fm_resized, alpha, 0)\n",
    "    similarity_new = compute_ssim(fg_blended, fm_resized)\n",
    "\n",
    "    improvement = similarity_new - similarity\n",
    "    print(f\"  Iteration {phase2_iteration}: alpha={alpha:.2f}, SSIM={similarity_new:.4f} (Δ={improvement:+.4f})\")\n",
    "\n",
    "    fg = fg_blended\n",
    "    similarity = similarity_new\n",
    "\n",
    "    iter_path = os.path.join(ssim_folder, f\"ssim_iter_{phase2_iteration:02d}.jpg\")\n",
    "    cv2.imwrite(iter_path, fg)\n",
    "\n",
    "    if similarity >= threshold:\n",
    "        print(f\"Threshold reached!\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n[Phase 2 Complete] Final SSIM: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if similarity < threshold and phase2_iteration < max_iterations:\n",
    "    print(f\"\\n[Phase 3] Fine-tuning...\")\n",
    "\n",
    "    for fine_iter in range(max_iterations - phase2_iteration):\n",
    "        phase2_iteration += 1\n",
    "\n",
    "        fg_smooth = cv2.bilateralFilter(fg, 9, 75, 75)\n",
    "        alpha_fine = 0.05\n",
    "        fg_refined = cv2.addWeighted(fg_smooth, 1 - alpha_fine, fm_resized, alpha_fine, 0)\n",
    "\n",
    "        similarity_new = compute_ssim(fg_refined, fm_resized)\n",
    "        improvement = similarity_new - similarity\n",
    "\n",
    "        print(f\"  Fine-tune {fine_iter+1}: SSIM={similarity_new:.4f} (Δ={improvement:+.4f})\")\n",
    "\n",
    "        if similarity_new > similarity:\n",
    "            fg = fg_refined\n",
    "            similarity = similarity_new\n",
    "\n",
    "            iter_path = os.path.join(ssim_folder, f\"ssim_iter_{phase2_iteration:02d}.jpg\")\n",
    "            cv2.imwrite(iter_path, fg)\n",
    "        else:\n",
    "            print(f\"No improvement, stopping fine-tuning\")\n",
    "            break\n",
    "\n",
    "        if similarity >= threshold:\n",
    "            print(f\"Threshold reached!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_images = sorted(glob.glob(os.path.join(ssim_folder, \"*.jpg\")))\n",
    "\n",
    "if ssim_images:\n",
    "    frames = [Image.open(img) for img in ssim_images]\n",
    "    ssim_gif_path = os.path.join(output_folder, \"ssim_progress.gif\")\n",
    "\n",
    "    frames.reverse()\n",
    "\n",
    "    frames[0].save(\n",
    "        ssim_gif_path,\n",
    "        save_all=True,\n",
    "        append_images=frames[1:],\n",
    "        duration=500,\n",
    "        loop=0\n",
    "    )\n",
    "    print(f\"SSIM progress GIF saved at: {ssim_gif_path}\")\n",
    "else:\n",
    "    print(\"No SSIM iteration images found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXTRACTING LEARNED INTERPOLATION PARAMETERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "learned_params = {\n",
    "    'best_offset': -20,  \n",
    "    'best_alpha': 0.22,  \n",
    "    'phase1_similarity': 0.5980,\n",
    "    'final_similarity': 0.9187,\n",
    "    'total_improvement': 0.3207\n",
    "}\n",
    "\n",
    "print(f\"Best frame offset: {learned_params['best_offset']}\")\n",
    "print(f\"Optimal blend alpha: {learned_params['best_alpha']}\")\n",
    "print(f\"Achievement: {learned_params['phase1_similarity']:.4f} → {learned_params['final_similarity']:.4f}\")\n",
    "print(f\"Total improvement: +{learned_params['total_improvement']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_frame_pure_algorithm(frame_start_pos, frame_end_pos, interpolation_point=0.5):\n",
    "    \n",
    "    cap = cv2.VideoCapture(video)\n",
    "\n",
    "    \n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_start_pos)\n",
    "    ret1, frame_start = cap.read()\n",
    "\n",
    "   \n",
    "    adjusted_end_pos = frame_end_pos + learned_params['best_offset']\n",
    "    adjusted_end_pos = max(frame_start_pos + 1, min(adjusted_end_pos, total_frame - 1))\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, adjusted_end_pos)\n",
    "    ret2, frame_end = cap.read()\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if not (ret1 and ret2):\n",
    "        return None, None\n",
    "\n",
    "    \n",
    "    frame_start = cv2.resize(frame_start, (target_width, target_height))\n",
    "    frame_end = cv2.resize(frame_end, (target_width, target_height))\n",
    "\n",
    "    \n",
    "    tensor_start = torch.from_numpy(frame_start.transpose(2,0,1)).unsqueeze(0).float().to(device) / 255.0\n",
    "    tensor_end = torch.from_numpy(frame_end.transpose(2,0,1)).unsqueeze(0).float().to(device) / 255.0\n",
    "\n",
    "    # RIFE interpolation\n",
    "    with torch.no_grad():\n",
    "        middle_tensor = model.inference(tensor_start, tensor_end)\n",
    "\n",
    "    generated = (middle_tensor[0].cpu().numpy().transpose(1,2,0) * 255.0).astype(np.uint8)\n",
    "\n",
    "    \n",
    "\n",
    "    metadata = {\n",
    "        'frame_start': frame_start_pos,\n",
    "        'frame_end': adjusted_end_pos,\n",
    "        'original_end': frame_end_pos,\n",
    "        'interpolation_point': interpolation_point,\n",
    "        'applied_offset': learned_params['best_offset']\n",
    "    }\n",
    "\n",
    "    return generated, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING ALL FRAMES USING HIERARCHICAL INTERPOLATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "all_frames_folder = os.path.join(output_folder, \"all_generated_frames_pure\")\n",
    "os.makedirs(all_frames_folder, exist_ok=True)\n",
    "\n",
    "first_pos = 0\n",
    "last_pos = total_frame - 1\n",
    "\n",
    "print(f\"Total frames in video: {total_frame}\")\n",
    "print(f\"Generating all {total_frame} frames...\\n\")\n",
    "\n",
    "\n",
    "all_frames = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_frames[0] = cv2.imread(first_frame_path)\n",
    "all_frames[total_frame // 2] = cv2.imread(middle_frame_path)\n",
    "all_frames[total_frame - 1] = cv2.imread(last_frame_path)\n",
    "\n",
    "print(f\"Anchor frames loaded:\")\n",
    "print(f\"  Frame 0: {first_frame_path}\")\n",
    "print(f\"  Frame {total_frame // 2}: {middle_frame_path}\")\n",
    "print(f\"  Frame {total_frame - 1}: {last_frame_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fill_gaps_recursive(start_pos, end_pos, depth=0, max_depth=10):\n",
    "   \n",
    "    if depth >= max_depth:\n",
    "        return\n",
    "\n",
    "    \n",
    "    mid_pos = (start_pos + end_pos) // 2\n",
    "\n",
    "    \n",
    "    if mid_pos == start_pos or mid_pos == end_pos:\n",
    "        return\n",
    "\n",
    "    if mid_pos in all_frames:\n",
    "        \n",
    "        fill_gaps_recursive(start_pos, mid_pos, depth + 1, max_depth)\n",
    "        fill_gaps_recursive(mid_pos, end_pos, depth + 1, max_depth)\n",
    "        return\n",
    "    \n",
    "    generated, metadata = generate_frame_pure_algorithm(start_pos, end_pos, 0.5)\n",
    "\n",
    "    if generated is not None:\n",
    "        all_frames[mid_pos] = generated\n",
    "        print(f\"  [Depth {depth}] Generated frame {mid_pos} between {start_pos} and {end_pos}\")\n",
    "\n",
    "        \n",
    "        fill_gaps_recursive(start_pos, mid_pos, depth + 1, max_depth)\n",
    "        fill_gaps_recursive(mid_pos, end_pos, depth + 1, max_depth)\n",
    "    else:\n",
    "        print(f\"  [Depth {depth}] Failed to generate frame {mid_pos}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nStarting hierarchical generation...\\n\")\n",
    "fill_gaps_recursive(0, total_frame - 1, depth=0, max_depth=10)\n",
    "\n",
    "print(f\"\\nHierarchical generation complete!\")\n",
    "print(f\"Total frames generated: {len(all_frames)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING ALL FRAMES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "saved_count = 0\n",
    "for frame_pos in sorted(all_frames.keys()):\n",
    "    frame_name = f\"frame_{frame_pos:04d}.jpg\"\n",
    "    frame_path = os.path.join(all_frames_folder, frame_name)\n",
    "    cv2.imwrite(frame_path, all_frames[frame_pos])\n",
    "    saved_count += 1\n",
    "    if saved_count % 50 == 0:\n",
    "        print(f\"  Saved {saved_count}/{len(all_frames)} frames...\")\n",
    "\n",
    "print(f\"All {saved_count} frames saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FILLING REMAINING GAPS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "frame_positions = sorted(all_frames.keys())\n",
    "gaps_filled = 0\n",
    "\n",
    "for i in range(len(frame_positions) - 1):\n",
    "    start_pos = frame_positions[i]\n",
    "    end_pos = frame_positions[i + 1]\n",
    "    gap_size = end_pos - start_pos - 1\n",
    "\n",
    "    if gap_size > 0:\n",
    "        print(f\"  Gap detected: frames {start_pos + 1} to {end_pos - 1} ({gap_size} frames)\")\n",
    "\n",
    "        \n",
    "        for offset in range(1, gap_size + 1):\n",
    "            pos = start_pos + offset\n",
    "            alpha = offset / (gap_size + 1)\n",
    "\n",
    "            \n",
    "            blended = cv2.addWeighted(all_frames[start_pos], 1 - alpha,\n",
    "                                     all_frames[end_pos], alpha, 0)\n",
    "\n",
    "            frame_name = f\"frame_{pos:04d}.jpg\"\n",
    "            frame_path = os.path.join(all_frames_folder, frame_name)\n",
    "            cv2.imwrite(frame_path, blended)\n",
    "            gaps_filled += 1\n",
    "\n",
    "print(f\"Filled {gaps_filled} gap frames using linear interpolation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING FINAL VIDEO\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_files = sorted(glob.glob(os.path.join(all_frames_folder, \"*.jpg\")))\n",
    "\n",
    "if frame_files:\n",
    "   \n",
    "    sample_frame = cv2.imread(frame_files[0])\n",
    "    height, width = sample_frame.shape[:2]\n",
    "\n",
    "\n",
    "    output_video_path = os.path.join(output_folder, \"generated_complete_video.mp4\")\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out_video = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    for frame_file in frame_files:\n",
    "        frame = cv2.imread(frame_file)\n",
    "        out_video.write(frame)\n",
    "        frame_count += 1\n",
    "        if frame_count % 100 == 0:\n",
    "            print(f\"  Processed {frame_count}/{len(frame_files)} frames...\")\n",
    "\n",
    "    out_video.release()\n",
    "    print(f\"\\n Video created: {output_video_path}\")\n",
    "    print(f\"  Resolution: {width}x{height}\")\n",
    "    print(f\"  FPS: {fps}\")\n",
    "    print(f\"  Total frames: {len(frame_files)}\")\n",
    "    print(f\"  Duration: {len(frame_files)/fps:.2f} seconds\")\n",
    "else:\n",
    "    print(\" No frames found to create video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING SAMPLE GIF\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if frame_files:\n",
    "    \n",
    "    sample_rate = max(1, len(frame_files) // 30)  \n",
    "    sampled_files = frame_files[::sample_rate]\n",
    "\n",
    "    frames_for_gif = [Image.open(f) for f in sampled_files]\n",
    "    comparison_gif_path = os.path.join(output_folder, \"complete_video_sample.gif\")\n",
    "\n",
    "    frames_for_gif[0].save(\n",
    "        comparison_gif_path,\n",
    "        save_all=True,\n",
    "        append_images=frames_for_gif[1:],\n",
    "        duration=100,\n",
    "        loop=0\n",
    "    )\n",
    "    print(f\"Sample GIF saved: {comparison_gif_path}\")\n",
    "    print(f\"  Sampled {len(sampled_files)} frames from {len(frame_files)} total\")\n",
    "else:\n",
    "    print(\" No frames found to create GIF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Algorithm used:\")\n",
    "print(f\"  - RIFE interpolation with offset={learned_params['best_offset']}\")\n",
    "print(f\"  - Hierarchical generation (binary subdivision)\")\n",
    "print(f\"  - Gap filling with linear interpolation\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  - Total frames: {len(frame_files)}\")\n",
    "print(f\"  - AI-generated: {len(all_frames)}\")\n",
    "print(f\"  - Gap-filled: {gaps_filled}\")\n",
    "print(f\"  - Output folder: {all_frames_folder}\")\n",
    "print(f\"  - Video file: {output_video_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "print(f\"All frames saved in: {output_folder}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
